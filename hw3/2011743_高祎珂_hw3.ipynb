{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入需要的包\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import re,string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读出诗名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles=os.listdir('dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读出作者和内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_paths=[os.path.join('dataset',title)for title in titles]\n",
    "authors=[]\n",
    "contexts=[]\n",
    "for i in range(len(doc_paths)):\n",
    "    with open(doc_paths[i]) as f:\n",
    "        # for line in f.readlines:\n",
    "        authors.append(f.readline())\n",
    "        contexts.append(f.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(titles)):\n",
    "    titles[i]=titles[i].strip('.txt')\n",
    "titles = [x.lower() for x in titles]\n",
    "for i in range(len(authors)):\n",
    "    authors[i]=authors[i].strip('\\n')\n",
    "    authors[i]=authors[i].strip('Author: ')\n",
    "authors = [x.lower() for x in authors]\n",
    "remove_punct_map = dict.fromkeys(map(ord, re.sub('\\'','',string.punctuation)))\n",
    "for i in range(len(contexts)):\n",
    "    contexts[i]=contexts[i].replace('\\n',' ')\n",
    "    new_str=\"\"\n",
    "    for word in contexts[i].split(' '):\n",
    "        new_str+=word.translate(remove_punct_map)\n",
    "        new_str=new_str+' '\n",
    "    contexts[i]=new_str\n",
    "        \n",
    "contexts = [x.lower() for x in contexts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对于这三个作用域建立词袋 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 改变大小写，并进行排序\n",
    "def sort_str(wordSet):\n",
    "    listtemp = [x.lower() for x in wordSet]\n",
    "    # 对元组排序，因为元组为：（忽略大小写的字符串，字符串），就是按忽略大小写的字符串排序\n",
    "    listtemp.sort()   \n",
    "    return listtemp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 作者词袋\n",
    "# 一个作者看成一个整体，对准确性要求太高\n",
    "# author_wordset=set(authors)\n",
    "# author_wordset=sort_str_author(author_wordset)\n",
    "author_wordset=set(authors[0].split(' '))\n",
    "for i in range(len(authors)-1):\n",
    "    author_wordset=set(author_wordset).union(authors[i+1].split(' '))\n",
    "# print(title_wordset)\n",
    "author_wordset=sort_str(author_wordset)\n",
    "\n",
    "# 标题词袋\n",
    "title_wordset=set(titles[0].split(' '))\n",
    "for i in range(len(titles)-1):\n",
    "    title_wordset=set(title_wordset).union(titles[i+1].split(' '))\n",
    "# print(title_wordset)\n",
    "title_wordset=sort_str(title_wordset)\n",
    "# print(title_wordset)\n",
    "\n",
    "# 正文词袋\n",
    "text_wordset=set(contexts[0].split(' '))\n",
    "for i in range(len(contexts)-1):\n",
    "    text_wordset=set(text_wordset).union(contexts[i+1].split(' '))\n",
    "# new_set=set()\n",
    "# # 去掉标点    可以考虑一下‘s，这个不删\n",
    "# for text in text_wordset:\n",
    "#     remove_punct_map = dict.fromkeys(map(ord, re.sub('\\'','',string.punctuation)))\n",
    "#     new_set.add(text.translate(remove_punct_map))\n",
    "# text_wordset=new_set\n",
    "# text_wordset.remove('')\n",
    "text_wordset=sort_str(text_wordset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.统计词项tj在文档Di中出现的次数，也就是词频。\n",
    "def computeTF(wordSet,area_list):\n",
    "    tf=[]\n",
    "    for i in range(len(area_list)):\n",
    "        tf.append(dict.fromkeys(wordSet, 0))\n",
    "    # print(tf[0])\n",
    "    for i in range(len(area_list)):\n",
    "        split=area_list[i].split(' ')\n",
    "        for word in split:\n",
    "            if word in wordSet:\n",
    "                tf[i][word] += 1\n",
    "    return tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.计算逆文档频率IDF\n",
    "def computeIDF(tfList): \n",
    "    idfDict = dict.fromkeys(tfList[0],0) #词为key，初始值为0\n",
    "    # print(\"1\",idfDict)\n",
    "    N = len(tfList)  #总文档数量 \n",
    "    for tf in tfList: # 遍历字典中每一篇文章\n",
    "        for word, count in tf.items(): #遍历当前文章的每一个词\n",
    "            if count > 0 : #当前遍历的词语在当前遍历到的文章中出现\n",
    "                idfDict[word] += 1 #包含词项tj的文档的篇数df+1  \n",
    "    for word, Ni in idfDict.items(): #利用公式将df替换为逆文档频率idf\n",
    "        idfDict[word] = math.log10(N/Ni)  #N,Ni均不会为0\n",
    "    return idfDict   #返回逆文档频率IDF字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.计算tf-idf(term frequency–inverse document frequency)\n",
    "def computeTFIDF(tflist, idfs): #tf词频,idf逆文档频率   \n",
    "    tfidf_list = []\n",
    "    for i in range(len(tflist)):\n",
    "        tf=tflist[i]\n",
    "        tfidf_list.append({})\n",
    "        for word, tfval in tf.items():\n",
    "            tfidf_list[i][word] = tfval * idfs[word]\n",
    "    return tfidf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_title=computeTF(title_wordset,titles)\n",
    "tf_author=computeTF(author_wordset,authors)\n",
    "tf_text=computeTF(text_wordset,contexts)\n",
    "idfs_title=computeIDF(tf_title)\n",
    "idfs_author=computeIDF(tf_author)\n",
    "idfs_text=computeIDF(tf_text)\n",
    "tfidf_title=computeTFIDF(tf_title,idfs_title)\n",
    "tfidf_author=computeTFIDF(tf_author,idfs_author)\n",
    "tfidf_text=computeTFIDF(tf_text,idfs_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_same(query,idf,wordset,tfidf_list):\n",
    "    result=[]\n",
    "    if query==\"\":\n",
    "        for i in range(len(tfidf_list)):\n",
    "            result.append(0)\n",
    "    query=query.strip('\\n')\n",
    "    new_str=\"\"\n",
    "    for word in query.split(' '):\n",
    "        word = word.lower() \n",
    "        new_str+=word.translate(remove_punct_map)\n",
    "        new_str=new_str+' '\n",
    "    # 去掉最后一个空格\n",
    "    new_str=new_str.strip(' ')\n",
    "    query=new_str\n",
    "    query_list=[]\n",
    "    query_list.append(query)\n",
    "    query=query_list\n",
    "    # print(query)\n",
    "    tf_q=computeTF(wordset,query)\n",
    "    tfidf_q=computeTFIDF(tf_q,idf)\n",
    "    # print(tfidf_q)\n",
    "    # 计算余弦相似度，不要分母\n",
    "    for i in range(len(tfidf_list)):\n",
    "        temp=0\n",
    "        # print(i,type(tfidf_list[i]))\n",
    "        for word in tfidf_list[i].keys():\n",
    "            temp+=tfidf_list[i][word]*tfidf_q[0][word]\n",
    "        result.append(temp)\n",
    "        # print(\"end\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freedom\n",
      "tonight i can write\n",
      "where the mind is without fear\n",
      "aedh wishes for the cloths of heaven\n",
      "down by the salley gardens\n",
      "when you are old\n",
      "walk with me in moonligh\n",
      "a drinking song\n",
      "leave this\n"
     ]
    }
   ],
   "source": [
    "# q类型为“author:A,title:B,”\n",
    "# q=input(\"qingshuru1\")\n",
    "# q=\"Rabindranath Tagore\"\n",
    "# cos_same(q,idfs_author,author_wordset,tfidf_author)\n",
    "query_title=input(\"诗名\")\n",
    "query_author=input(\"作者\")\n",
    "query_text=input(\"正文\")\n",
    "title_result=cos_same(query_title,idfs_title,title_wordset,tfidf_title)\n",
    "author_result=cos_same(query_author,idfs_author,author_wordset,tfidf_author)\n",
    "text_result=cos_same(query_text,idfs_text,text_wordset,tfidf_text)\n",
    "result=[]\n",
    "for i in range(len(titles)):\n",
    "    result.append(title_result[i]+author_result[i]+text_result[i])\n",
    "zero_id=[]\n",
    "for i in range(len(result)):\n",
    "    if result[i]==0:\n",
    "        zero_id.append(i)\n",
    "\n",
    "sorted_id = sorted(range(len(result)), key=lambda k: result[k], reverse=True)\n",
    "for i in sorted_id:\n",
    "    if i not in zero_id:\n",
    "        print(titles[i])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "865d8b2eb28e274047ba64063dfb6a2aabf0dfec4905d304d7a76618dae6fdd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
